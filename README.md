# epubAutoSave
自动从某些网站爬取文件并在一定的操作后生成固定样式的epub

没有面向对象的部分，想要使用的话请自行配置环境，自修改的epublib我已经附上了

# 功能

实现在非允许情况下，自动爬取lk上的轻小说，使用的用户仅保留了token等网站验证信息

已经能够绕开lk的 **勇者权限** **轻币支付**

我开始开发新工具之后组里没人维护了，于是直接发来github了

如果有更新大概是我终于抓到苦力接班了

# 使用方法

比你预期的简单很大一截

### 主功能文件：

EPUB_Factory.py

```python
EPUB_static = {
    "WebIndex": 12345678,       # 当你在web上看的时候，网址里头那个数字（cid）
    "BookName": "",             # 你希望爬下来之后的书名
    "ImgName": "图片-",          # 默认保存图片名，屎山的来源之一
    "OtherName": "",            # 本来这里想作为epub制作者名字的位置的，但是会做e的人肯定都会改元数据，后面就TODO了
    "Translator": "",           # 如果你有点良心的话，记得把这里填一下，应该还是会写进元数据的，不过我好像哪次更新的时候大调了这块，有可能你写了也还是得自己调元数据
    "haveHTML": False,          # 打开你想看的lk的web，Ctrl+s得到的那个html文件，不过这里改了没用，要去改最下面运行那里
    'haveAPP': False            # 不教怎么使用app爬取功能，不过代码很简单，随便翻一翻应该就能学会的
}
```

只要填webIndex和BookName就能正常运行了

**跑起来之后，会停下来让你去检查目录**

目录文件在**./书名/WebIndex - Content.txt**那里，自己改，改了之后也要去旁边的那个ImgSave-Index文件里面对应调整好

调整完之后再在python运行界面敲除了“ok”以外的任意内容(\n不行)，随后出现的那个字典全部填满了就是全部目录都对应上了，就是跑通了，输入ok就可以结束程序了

**生成的epub在书名文件夹里面**

### 附功能文件：

EPUB_Series.py

```python
profile_ID = 358191          # 要被你爬的合集的作者的ID（是lk消息传递中的aid）
series_ID = 2023             # 要被你爬的合集的ID（sid）
```

主功能还简单点，就填这俩就能爬lk的合集了

不用调目录，直接以合集内每个帖子的名字为目录

# 后记

lk那个不知道怎么形容比较好的框架

作为一个写爬虫的人，觉得他的框架比较脆 或者说易于攻击？

应该是基于php写的，消息传递用的是很基础的post、明文传输，前端的vue没起到辅助防御的效果，图片也很容易能扒下来

唯一麻烦点的就是之前好像lk被攻击了，换了个防爬的防火墙，然后我用了个更好用的工具，整洁了一部分代码

代码里面大部分都不是爬虫，主要时间空间复杂度是在文本流处理和epub生成上

想看屎山的话可以随便看看，我在写新的模块化版本了（咕咕咕？~）

想一起拆别的网址可以帮我写点更新，用git推送的那个信息来交流（笑）
